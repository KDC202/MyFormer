{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.utils import degree\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,   # 输入token的dim\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class TransFormer(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, depth, num_heads,dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, num_heads=num_heads, qkv_bias=True,attn_drop_ratio=dropout,proj_drop_ratio=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, hidden_dim, dropout=dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def full_attention_conv(qs, ks, vs, kernel, output_attn=False):\n",
    "    '''\n",
    "    qs: query tensor [N, H, M]\n",
    "    ks: key tensor [L, H, M]\n",
    "    vs: value tensor [L, H, D]\n",
    "\n",
    "    return output [N, H, D]\n",
    "    '''\n",
    "    if kernel == 'simple':\n",
    "        # normalize input\n",
    "        qs = qs / torch.norm(qs, p=2) # [N, H, M]\n",
    "        ks = ks / torch.norm(ks, p=2) # [L, H, M]\n",
    "        N = qs.shape[0]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\n",
    "        attention_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs) # [N, H, D]\n",
    "        all_ones = torch.ones([vs.shape[0]]).to(vs.device)\n",
    "        vs_sum = torch.einsum(\"l,lhd->hd\", all_ones, vs) # [H, D]\n",
    "        attention_num += vs_sum.unsqueeze(0).repeat(vs.shape[0], 1, 1) # [N, H, D]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\n",
    "        attention_normalizer = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        attn_output = attention_num / attention_normalizer # [N, H, D]\n",
    "\n",
    "        # compute attention for visualization if needed\n",
    "        if output_attn:\n",
    "            attention = torch.einsum(\"nhm,lhm->nlh\", qs, ks) / attention_normalizer # [N, L, H]\n",
    "\n",
    "    elif kernel == 'sigmoid':\n",
    "        # numerator\n",
    "        attention_num = torch.sigmoid(torch.einsum(\"nhm,lhm->nlh\", qs, ks))  # [N, L, H]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        attention_normalizer = torch.einsum(\"nlh,l->nh\", attention_num, all_ones)\n",
    "        attention_normalizer = attention_normalizer.unsqueeze(1).repeat(1, ks.shape[0], 1)  # [N, L, H]\n",
    "\n",
    "        # compute attention and attentive aggregated results\n",
    "        attention = attention_num / attention_normalizer\n",
    "        attn_output = torch.einsum(\"nlh,lhd->nhd\", attention, vs)  # [N, H, D]\n",
    "\n",
    "    if output_attn:\n",
    "        return attn_output, attention\n",
    "    else:\n",
    "        return attn_output\n",
    "\n",
    "def gcn_conv(x, edge_index, edge_weight):\n",
    "    N, H = x.shape[0], x.shape[1]\n",
    "    row, col = edge_index\n",
    "    d = degree(col, N).float()\n",
    "    d_norm_in = (1. / d[col]).sqrt()\n",
    "    d_norm_out = (1. / d[row]).sqrt()\n",
    "    gcn_conv_output = []\n",
    "    if edge_weight is None:\n",
    "        value = torch.ones_like(row) * d_norm_in * d_norm_out\n",
    "    else:\n",
    "        value = edge_weight * d_norm_in * d_norm_out\n",
    "    value = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    adj = SparseTensor(row=col, col=row, value=value, sparse_sizes=(N, N))\n",
    "    for i in range(x.shape[1]):\n",
    "        gcn_conv_output.append( matmul(adj, x[:, i]) )  # [N, D]\n",
    "    gcn_conv_output = torch.stack(gcn_conv_output, dim=1) # [N, H, D]\n",
    "    return gcn_conv_output\n",
    "\n",
    "class DIFFormerConv(nn.Module):\n",
    "    '''\n",
    "    one DIFFormer layer\n",
    "    '''\n",
    "    def __init__(self, in_channels,\n",
    "               out_channels,\n",
    "               num_heads,\n",
    "               kernel='simple',\n",
    "               use_graph=True,\n",
    "               use_weight=True):\n",
    "        super(DIFFormerConv, self).__init__()\n",
    "        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        if use_weight:\n",
    "            self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.kernel = kernel\n",
    "        self.use_graph = use_graph\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, edge_index=None, edge_weight=None, output_attn=False):\n",
    "        # feature transformation\n",
    "        query = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        key = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        if self.use_weight:\n",
    "            value = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        else:\n",
    "            value = source_input.reshape(-1, 1, self.out_channels)\n",
    "\n",
    "        # compute full attentive aggregation\n",
    "        if output_attn:\n",
    "            attention_output, attn = full_attention_conv(query, key, value, self.kernel, output_attn)  # [N, H, D]\n",
    "        else:\n",
    "            attention_output = full_attention_conv(query,key,value,self.kernel) # [N, H, D]\n",
    "\n",
    "        # use input graph for gcn conv\n",
    "        if self.use_graph:\n",
    "            final_output = attention_output + gcn_conv(value, edge_index, edge_weight)\n",
    "        else:\n",
    "            final_output = attention_output\n",
    "        final_output = final_output.mean(dim=1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attn\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "class DIFFormer(nn.Module):\n",
    "    '''\n",
    "    DIFFormer model class\n",
    "    x: input node features [N, D]\n",
    "    edge_index: 2-dim indices of edges [2, E]\n",
    "    return y_hat predicted logits [N, C]\n",
    "    '''\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1, kernel='simple',\n",
    "                 alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):\n",
    "        super(DIFFormer, self).__init__()\n",
    "        print(\"in_channels=\",in_channels)\n",
    "        print(\"out_channels=\",out_channels)``\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                DIFFormerConv(hidden_channels, hidden_channels, num_heads=num_heads, kernel=kernel, use_graph=use_graph, use_weight=use_weight))\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "\n",
    "        self.fcs.append(nn.Linear(hidden_channels, out_channels))\n",
    "        \n",
    "        # 图特征\n",
    "        self.fcs2 = nn.ModuleList()\n",
    "        self.bns2 = nn.ModuleList()\n",
    "        self.fcs2.append(nn.Linear(6, out_channels))\n",
    "        self.bns2.append(nn.LayerNorm(out_channels))\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, 128))\t\t\t\t\t# nn.Parameter()定义可学习参数\n",
    "        self.transform = TransFormer(out_channels, hidden_channels, depth=8, num_heads=8, dropout=dropout)\n",
    "        self.fcs.append(nn.Linear(out_channels, 1))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.activation = F.relu\n",
    "        self.use_bn = use_bn\n",
    "        self.residual = use_residual\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "        for bn in self.bns2:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs2:\n",
    "            fc.reset_parameters()\n",
    "\n",
    "    def forward(self, x, now, edge_index=None, edge_weight=None):\n",
    "        layer_ = []\n",
    "        # input MLP layer\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        print(\"fcs[0]x.shape\",x.shape)\n",
    "        # store as residual link\n",
    "        layer_.append(x)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # graph convolution with DIFFormer layer\n",
    "            x = conv(x, x, edge_index, edge_weight)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1-self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i+1](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            layer_.append(x)\n",
    "\n",
    "        # output MLP layer(num_node,node_feat)\n",
    "        x_out = self.fcs[-2](x)\n",
    "        # 图特征处理(batch_size,node_feat)\n",
    "        now = self.fcs2[0](now)\n",
    "        now = self.bns2[0](now)\n",
    "        # 特征融合\n",
    "        repeat_x = x_out.repeat(now.size(0), 1, 1)\n",
    "        additional_now = now.unsqueeze(1)\n",
    "        result = repeat_x + additional_now\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=now.size(0))  # self.cls_token: (1, 1, dim) -> cls_tokens: (batchSize, 1, dim)\n",
    "        # print(cls_tokens)\n",
    "        # print(cls_tokens.shape)\n",
    "        print(\"拼接前\",result.shape)\n",
    "        x = torch.cat((cls_tokens, result), dim=1)               # 将cls_token拼接到patch token中去        \n",
    "        print(\"拼接后\",x.shape)\n",
    "        x = self.transform(x)\n",
    "        print(\"afterTransform\",x.shape)\n",
    "        # print(x)\n",
    "        x = x[:,0]\n",
    "        # print(\"------------------\")\n",
    "        # print(x)\n",
    "        print(\"x[:,0]\",x.shape)\n",
    "        x_out = self.fcs[-1](x)\n",
    "        return x_out\n",
    "\n",
    "    def get_attentions(self, x):\n",
    "        layer_, attentions = [], []\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        layer_.append(x)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x, attn = conv(x, x, output_attn=True)\n",
    "            attentions.append(attn)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1 - self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i + 1](x)\n",
    "            layer_.append(x)\n",
    "        return torch.stack(attentions, dim=0) # [layer num, N, N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小 torch.Size([128, 32])\n",
      "图特征大小 torch.Size([64, 6])\n",
      "in_channels= 32\n",
      "out_channels= 128\n",
      "fcs[0]x.shape torch.Size([128, 512])\n",
      "拼接前 torch.Size([64, 128, 128])\n",
      "拼接后 torch.Size([64, 129, 128])\n",
      "afterTransform torch.Size([64, 129, 128])\n",
      "x[:,0] torch.Size([64, 128])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1, kernel='simple',\n",
    "#                  alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):\n",
    "device = torch.device(\"cuda:\" + str(0)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "size = (128,32)\n",
    "data = torch.randn(size).to(device)\n",
    "print(\"数据集大小\",data.shape)\n",
    "# print(data)\n",
    "size2 = (64,6)\n",
    "data2 = torch.randn(size2).to(device)\n",
    "print(\"图特征大小\",data2.shape)\n",
    "model=DIFFormer(data.shape[1],hidden_channels=512, out_channels=128, num_layers=2, alpha=0.5, dropout=0.5, num_heads=8, kernel=\"simple\",\n",
    "                       use_bn=True, use_residual=True, use_graph=False, use_weight=True).to(device)\n",
    "out = model(data,data2, None)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
