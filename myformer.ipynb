{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.utils import degree\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_attention_conv(qs, ks, vs, kernel, output_attn=False):\n",
    "    '''\n",
    "    qs: query tensor [N, H, M]\n",
    "    ks: key tensor [L, H, M]\n",
    "    vs: value tensor [L, H, D]\n",
    "\n",
    "    return output [N, H, D]\n",
    "    '''\n",
    "    if kernel == 'simple':\n",
    "        # normalize input\n",
    "        qs = qs / torch.norm(qs, p=2) # [N, H, M]\n",
    "        ks = ks / torch.norm(ks, p=2) # [L, H, M]\n",
    "        N = qs.shape[0]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\n",
    "        attention_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs) # [N, H, D]\n",
    "        all_ones = torch.ones([vs.shape[0]]).to(vs.device)\n",
    "        vs_sum = torch.einsum(\"l,lhd->hd\", all_ones, vs) # [H, D]\n",
    "        attention_num += vs_sum.unsqueeze(0).repeat(vs.shape[0], 1, 1) # [N, H, D]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\n",
    "        attention_normalizer = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        attn_output = attention_num / attention_normalizer # [N, H, D]\n",
    "\n",
    "        # compute attention for visualization if needed\n",
    "        if output_attn:\n",
    "            attention = torch.einsum(\"nhm,lhm->nlh\", qs, ks) / attention_normalizer # [N, L, H]\n",
    "\n",
    "    elif kernel == 'sigmoid':\n",
    "        # numerator\n",
    "        attention_num = torch.sigmoid(torch.einsum(\"nhm,lhm->nlh\", qs, ks))  # [N, L, H]\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        attention_normalizer = torch.einsum(\"nlh,l->nh\", attention_num, all_ones)\n",
    "        attention_normalizer = attention_normalizer.unsqueeze(1).repeat(1, ks.shape[0], 1)  # [N, L, H]\n",
    "\n",
    "        # compute attention and attentive aggregated results\n",
    "        attention = attention_num / attention_normalizer\n",
    "        attn_output = torch.einsum(\"nlh,lhd->nhd\", attention, vs)  # [N, H, D]\n",
    "\n",
    "    if output_attn:\n",
    "        return attn_output, attention\n",
    "    else:\n",
    "        return attn_output\n",
    "\n",
    "def gcn_conv(x, edge_index, edge_weight):\n",
    "    N, H = x.shape[0], x.shape[1]\n",
    "    # print(N)\n",
    "    print(\"edge_index\",edge_index.shape)\n",
    "    row, col = edge_index\n",
    "    # print(row)\n",
    "    d = degree(col, N).float()\n",
    "    d_norm_in = (1. / d[col]).sqrt()\n",
    "    d_norm_out = (1. / d[row]).sqrt()\n",
    "    \n",
    "    gcn_conv_output = []\n",
    "    \n",
    "    if edge_weight is None:\n",
    "        value = torch.ones_like(row) * d_norm_in * d_norm_out\n",
    "        \n",
    "    else:\n",
    "        value = edge_weight * d_norm_in * d_norm_out\n",
    "    value = torch.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "   \n",
    "    adj = SparseTensor(row=col, col=row, value=value, sparse_sizes=(N, N))\n",
    "    # print(adj)\n",
    "    # adj.to('cpu')\n",
    "    for i in range(x.shape[1]):\n",
    "        gcn_conv_output.append(matmul(adj, x[:, i]) )  # [N, D]\n",
    "        # print(\"here1\")\n",
    "    gcn_conv_output = torch.stack(gcn_conv_output, dim=1) # [N, H, D]\n",
    "    \n",
    "    return gcn_conv_output\n",
    "\n",
    "class DIFFormerConv(nn.Module):\n",
    "    '''\n",
    "    one DIFFormer layer\n",
    "    '''\n",
    "    def __init__(self, in_channels,\n",
    "               out_channels,\n",
    "               num_heads,\n",
    "               kernel='simple',\n",
    "               use_graph=True,\n",
    "               use_weight=True):\n",
    "        super(DIFFormerConv, self).__init__()\n",
    "        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        if use_weight:\n",
    "            self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.kernel = kernel\n",
    "        self.use_graph = use_graph\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, edge_index=None, edge_weight=None, output_attn=False):\n",
    "        # feature transformation\n",
    "        query = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        key = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        if self.use_weight:\n",
    "            value = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        else:\n",
    "            value = source_input.reshape(-1, 1, self.out_channels)\n",
    "\n",
    "        # compute full attentive aggregation\n",
    "        if output_attn:\n",
    "            attention_output, attn = full_attention_conv(query, key, value, self.kernel, output_attn)  # [N, H, D]\n",
    "        else:\n",
    "            attention_output = full_attention_conv(query,key,value,self.kernel) # [N, H, D]\n",
    "\n",
    "        # use input graph for gcn conv\n",
    "        if self.use_graph:\n",
    "            final_output = attention_output + gcn_conv(value, edge_index, edge_weight)\n",
    "        else:\n",
    "            final_output = attention_output\n",
    "        final_output = final_output.mean(dim=1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attn\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "class DIFFormer(nn.Module):\n",
    "    '''\n",
    "    DIFFormer model class\n",
    "    x: input node features [N, D]\n",
    "    edge_index: 2-dim indices of edges [2, E]\n",
    "    return y_hat predicted logits [N, C]\n",
    "    '''\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1, kernel='simple',\n",
    "                 alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):\n",
    "        \n",
    "        super(DIFFormer, self).__init__()\n",
    "        \n",
    "        # print(\"in:\",in_channels)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                DIFFormerConv(hidden_channels, hidden_channels, num_heads=num_heads, kernel=kernel, use_graph=use_graph, use_weight=use_weight))\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "\n",
    "        self.fcs.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.relu\n",
    "        self.use_bn = use_bn\n",
    "        self.residual = use_residual\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        layer_ = []\n",
    "        # print(x.shape)\n",
    "        # print(edge_index)\n",
    "        # input MLP layer\n",
    "        x = self.fcs[0](x)\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # store as residual link\n",
    "        layer_.append(x)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # graph convolution with DIFFormer layer\n",
    "            x = conv(x, x, edge_index, edge_weight)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1-self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i+1](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            layer_.append(x)\n",
    "\n",
    "        # output MLP layer\n",
    "        x_out = self.fcs[-1](x)\n",
    "        return x_out\n",
    "\n",
    "    def get_attentions(self, x):\n",
    "        layer_, attentions = [], []\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        layer_.append(x)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x, attn = conv(x, x, output_attn=True)\n",
    "            attentions.append(attn)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1 - self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i + 1](x)\n",
    "            layer_.append(x)\n",
    "        return torch.stack(attentions, dim=0) # [layer num, N, N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小 torch.Size([128, 32])\n",
      "图特征大小 torch.Size([64, 6])\n",
      "in: 32\n",
      "torch.Size([128, 32])\n",
      "None\n",
      "torch.Size([128, 512])\n",
      "tensor([[ 0.5305,  0.2127,  0.0791,  ..., -0.3409,  0.4752, -0.6582],\n",
      "        [ 0.7388, -0.1885, -0.6768,  ...,  1.1323,  0.2417,  1.2532],\n",
      "        [-0.1485,  0.3718, -0.4346,  ...,  0.0753, -0.1904,  0.7740],\n",
      "        ...,\n",
      "        [-0.2066, -0.1274, -1.4478,  ...,  0.5448, -0.6214,  0.5695],\n",
      "        [ 0.9115,  0.4818, -1.0217,  ...,  1.4887, -1.1908,  1.4185],\n",
      "        [-0.0199, -0.7302, -0.1347,  ...,  0.1038,  0.0548, -0.0278]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "# def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1, kernel='simple',\n",
    "#                  alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):\n",
    "device = torch.device(\"cuda:\" + str(0)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "size = (128,32)\n",
    "data = torch.randn(size).to(device)\n",
    "print(\"数据集大小\",data.shape)\n",
    "# print(data)\n",
    "size2 = (64,6)\n",
    "data2 = torch.randn(size2).to(device)\n",
    "print(\"图特征大小\",data2.shape)\n",
    "model=DIFFormer(data.shape[1],hidden_channels=512, out_channels=128, num_layers=2, alpha=0.5, dropout=0.5, num_heads=8, kernel=\"simple\",\n",
    "                       use_bn=True, use_residual=True, use_graph=False, use_weight=True).to(device)\n",
    "out = model(data,None, None)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_in_channels,\n",
    "                 graph_in_channels,\n",
    "                 dif_hidden_channels,\n",
    "                 hidden_channels,\n",
    "                 dif_out_channels,\n",
    "                 out_channels,\n",
    "                 num_layers=2,\n",
    "                 num_heads=8,\n",
    "                 alpha=0.5,\n",
    "                 dropout=0.5,\n",
    "                 use_bn=True,\n",
    "                 use_residual=True,\n",
    "                 use_graph=False,\n",
    "                 use_weight=True):\n",
    "        super(myformer,self).__init__()\n",
    "        # print(\"in_channels=\",in_channels)\n",
    "        # print(\"out_channels=\",out_channels)\n",
    "        self.difformer = DIFFormer(node_in_channels,dif_hidden_channels, dif_out_channels, num_layers, alpha=alpha, dropout=dropout, num_heads=num_heads, kernel=\"simple\",\n",
    "                       use_bn=use_bn, use_residual=use_residual, use_graph=use_graph, use_weight=use_weight).to(device)\n",
    "        \n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(dif_out_channels, hidden_channels))\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        self.fcs.append(nn.Linear(graph_in_channels, hidden_channels))\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        self.fcs.append(nn.Linear(hidden_channels, out_channels))\n",
    "        self.bns.append(nn.LayerNorm(out_channels))\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 128))\t\t\t\t\t# nn.Parameter()定义可学习参数\n",
    "        \n",
    "        self.difformer2 = DIFFormer(dif_out_channels,dif_hidden_channels, dif_out_channels, 1, alpha=alpha, dropout=dropout, num_heads=num_heads, kernel=\"simple\",\n",
    "                       use_bn=use_bn, use_residual=use_residual, use_graph=True, use_weight=use_weight).to(device)\n",
    "        \n",
    "        \n",
    "        self.activation = F.relu\n",
    "        self.use_bn = use_bn\n",
    "    def forward(self, x, now):\n",
    "        n = x.size(0)\n",
    "        print(\"num_node\",n)\n",
    "        print(\"input node shape:\",x.shape)\n",
    "        print(\"input graph shape:\",now.shape)\n",
    "        x = self.difformer(x,None,None)\n",
    "        print(\"after difformer node shape:\",x.shape)\n",
    "        \n",
    "        x = torch.cat((self.cls_token, x), dim=0)\n",
    "        print(\"拼接后\",x.shape)\n",
    "        # 假设拼接的节点与其他所有节点相连\n",
    "        edge_list = torch.zeros((2, n), dtype=torch.int64)\n",
    "        edge_list[0] = torch.zeros(n, dtype=torch.int64)\n",
    "        edge_list[1] = torch.arange(1, n+1, dtype=torch.int64)\n",
    "        # row, col = edge_list\n",
    "        # print(row)\n",
    "        # print(\"edge_list\",edge_list)\n",
    "        \n",
    "        x = self.difformer2(x,edge_list,None)\n",
    "        # 取第一个向量表示整个数据集\n",
    "        x = x[0]\n",
    "        \n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        now = self.fcs[1](now)\n",
    "        if self.use_bn:\n",
    "            now = self.bns[1](now)\n",
    "            \n",
    "        # 特征融合\n",
    "        repeat_x = x.repeat(now.size(0), 1, 1)\n",
    "        additional_now = now.unsqueeze(1)\n",
    "        y = repeat_x + additional_now\n",
    "        print(\"after merge:\",y.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # y = y.mean(dim=2)\n",
    "        # sum_tensor = torch.sum(y, dim=1)  # 在第二个维度上求和，结果大小为 (b, d)\n",
    "        # mean_tensor = sum_tensor / x.size(1) \n",
    "        # print(\"after mean:\",mean_tensor.shape)\n",
    "        # result = self.fcs[-1](mean_tensor)\n",
    "        # if self.use_bn:\n",
    "        #     result = self.bns[-1](result)\n",
    "        return y\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # for conv in self.convs:\n",
    "        #     conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "        self.difformer.reset_parameters()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_node 128\n",
      "input node shape: torch.Size([128, 32])\n",
      "input graph shape: torch.Size([64, 6])\n",
      "after difformer node shape: torch.Size([128, 128])\n",
      "拼接后 torch.Size([129, 128])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "129\n",
      "edge_index torch.Size([2, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat.device().is_cpu() INTERNAL ASSERT FAILED at \"csrc/cpu/spmm_cpu.cpp\":16, please report a bug to PyTorch. mat must be CPU tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2123683/2746148522.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# # print(\"edge_list\",edge_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# edge_list.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sfy/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2123683/854363888.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, now)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# print(\"edge_list\",edge_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifformer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m# 取第一个向量表示整个数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sfy/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2123683/2082411626.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# graph convolution with DIFFormer layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sfy/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2123683/2082411626.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, source_input, edge_index, edge_weight, output_attn)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# use input graph for gcn conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgcn_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2123683/2082411626.py\u001b[0m in \u001b[0;36mgcn_conv\u001b[0;34m(x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mgcn_conv_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m  \u001b[0;31m# [N, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# print(\"here1\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mgcn_conv_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcn_conv_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [N, H, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sfy/lib/python3.8/site-packages/torch_sparse/matmul.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(src, other, reduce)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \"\"\"\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sfy/lib/python3.8/site-packages/torch_sparse/matmul.py\u001b[0m in \u001b[0;36mspmm\u001b[0;34m(src, other, reduce)\u001b[0m\n\u001b[1;32m     81\u001b[0m          reduce: str = \"sum\") -> torch.Tensor:\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mspmm_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspmm_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sfy/lib/python3.8/site-packages/torch_sparse/matmul.py\u001b[0m in \u001b[0;36mspmm_sum\u001b[0;34m(src, other)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcolptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     return torch.ops.torch_sparse.spmm_sum(row, rowptr, col, value, colptr,\n\u001b[0m\u001b[1;32m     28\u001b[0m                                            csr2csc, other)\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sfy/lib/python3.8/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat.device().is_cpu() INTERNAL ASSERT FAILED at \"csrc/cpu/spmm_cpu.cpp\":16, please report a bug to PyTorch. mat must be CPU tensor"
     ]
    }
   ],
   "source": [
    "# def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=1, kernel='simple',\n",
    "#                  alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_graph=True):\n",
    "device = torch.device(\"cuda:\" + str(0)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "size = (128,32)\n",
    "data = torch.randn(size).to(device)\n",
    "# print(\"数据集大小\",data.shape)\n",
    "# print(data)\n",
    "size2 = (64,6)\n",
    "data2 = torch.randn(size2).to(device)\n",
    "# print(\"图特征大小\",data2.shape)\n",
    "model=myformer(node_in_channels=data.size(1),\n",
    "               graph_in_channels=data2.size(1),\n",
    "               dif_hidden_channels=512,\n",
    "               hidden_channels=512,\n",
    "               dif_out_channels=128,\n",
    "               out_channels=1,\n",
    "               num_layers=2,\n",
    "               num_heads=8,\n",
    "               alpha=0.5,\n",
    "               dropout=0.5,\n",
    "               use_bn=True,\n",
    "               use_residual=True,\n",
    "               use_graph=False,\n",
    "               use_weight=True).to(device)\n",
    "n = data.size(0)\n",
    "# edge_list = torch.zeros((2, n), dtype=torch.int64)\n",
    "# edge_list[0] = torch.zeros(n, dtype=torch.int64)\n",
    "# edge_list[1] = torch.arange(1, n+1, dtype=torch.int64)\n",
    "# # print(\"edge_list\",edge_list)\n",
    "# edge_list.to(device)\n",
    "out = model(data,data2)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8335, -0.0871, -1.3135,  1.1532],\n",
      "         [ 0.5646, -0.2365, -0.7816,  0.0688],\n",
      "         [-0.1525,  2.2362, -0.2618, -0.9286]],\n",
      "\n",
      "        [[-1.9100, -0.0028, -0.0393,  0.2988],\n",
      "         [-0.1047, -0.4419, -1.7085, -0.1470],\n",
      "         [ 0.6696,  0.5725, -1.0773,  1.0037]]])\n",
      "Mean tensor shape: torch.Size([2, 4])\n",
      "Mean tensor values: tensor([[ 0.7485,  0.6375, -0.7856,  0.0978],\n",
      "        [-0.4484,  0.0426, -0.9417,  0.3852]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设有 2 组数据，每组数据有 3 个数据，每个数据有 4 个特征\n",
    "b = 2\n",
    "n = 3\n",
    "d = 4\n",
    "\n",
    "# 创建一个随机的 b*n*d 大小的张量作为示例数据\n",
    "tensor_array = torch.randn(b, n, d)\n",
    "print(tensor_array)\n",
    "# 在 n 维度上求和取平均\n",
    "sum_tensor = torch.sum(tensor_array, dim=1)  # 在第二个维度上求和，结果大小为 (b, d)\n",
    "mean_tensor = sum_tensor / n  # 取平均，注意这里的 n 是每组数据的数量\n",
    "\n",
    "print(\"Mean tensor shape:\", mean_tensor.shape)\n",
    "print(\"Mean tensor values:\", mean_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
